<!DOCTYPE html>
<html>

  <head>
    <title>Zensors</title>
    <link rel="stylesheet" type="text/css" href="styles/global.css">
    <link rel="stylesheet" type="text/css" href="styles/newstagram.css">
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  </head>

  <body>
	<div class="page-nav">
		<div class="page-nav-item left" onclick="openPage('chutzpow.html')">< CHUTZ-POW</div>
		<div class="page-nav-item center" onclick="openPage('index.html')">HOME</div>
		<div class="page-nav-item right" onclick="openPage('upark.html')">UPARK ></div>
	</div>
  	<div class="page-title">Zensors</div>
  	<div class="project-info">Spring 2018 | HCI Undergraduate Research | <a href="https://www.zensors.com/">Website</a> </div>
  	<div id="zensors" class="margin-small">
	  	<div class="content">
			<div class="summary"><div class="subheader2">Context</div>
				<p>“Smart” appliances with built-in cameras, such as the NestCam and Amazon Echo Show, are becoming more pervasive. They hold the promise of bringing high fidelity, contextually rich sensing into our homes, workplaces and neighborhoods. Despite recent and impressive advances, computer vision sys-tems are still limited in the types of questions they can answer, and more importantly, do not immediately generalize acrossdiverse human environments. </p>
				<p>In response, researchers have investigated hybrid crowd and AI-powered methods that collect human labels to bootstrap automatic processes. However,deployments have been small and mostly confined to institutional settings, leaving open questions about scalability and generality of the approach.</p>

			</div>
			
			<div class="summary">
				<div class="subheader2">Team</div>
					I worked with several developers and designers, most of whom were HCI Masters/PHD students.
				<div class="subheader2">Skills</div> 
					<li>Personas</li> 
					<li>Scenarios</li> 
					<li>User Stories</li> 
					<li>Wireframing</li> 
					<li>Prototyping</li> 
					<li>User Testing</li> 
				<div class="subheader2">Tools/Technology</div> 
					<li>Adobe XD</li>
					<li>Mechanical Turk</li>
					<li>React</li>
					<li>TypeScript</li>
					<li>HTML/CSS</li>
			</div>
			
			<div class="summary"><div class="subheader2">Problem Statement</div> 
				The current system relies on a crowdworker interface (similar to Mechanical Turk) for labeling images. The existing interface has several issues. First, there is no way to communicate what is wrong with an image or question when the crowdworker has low confidence. Second, the user gets very little feedback. Third, users are shown an entire uncropped (often low-quality) region of an image. 
			<div class="subheader2">Project Vision</div>
				In redesigning this interface, I wanted to address these issues and create a labeling interface that is simple to use, but also improves accuracy and speed of answering.
			</div>
	  	</div>

<!-- 	  	<div class="page-title">Final Product</div>
	  	<div>
	  		<div class="final-product-text">
	  			View the full prototype <a href="website" target="_blank">here!</a> Selected screens are below:
	  		</div>
	  		<img class="product-pic" src="whatever">
	  		<div class="caption">This is a caption</div>
	  	</div>

	  	<div class="page-title">Process</div>
	  	<div class="process-overview">

			<button class="accordion">Something</button>
			<div class="panel">
			  <p>Lorem ipsum...	
			</div>
	  	</div> -->

	  	<img src="pics/arrow.png" id="arrow-up" onclick="slideUp()">
	</div>
  </body>

  <script src="js/general.js"></script>

</html>